<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover" />
  <title>AR_EXAMPLE â€” Face Filter (MediaPipe Tasks Vision)</title>
  <style>
    html, body { margin:0; padding:0; background:#000; height:100%; overflow:hidden; }
    #stage { position:fixed; inset:0; }
    video, canvas { position:absolute; top:0; left:0; width:100%; height:100%; object-fit:cover; transform: scaleX(-1); }
    #ui { position:fixed; inset:0; display:flex; align-items:center; justify-content:center; color:#fff; background:#000; z-index:10; }
    #ui.hide { display:none; }
    button { padding:12px 16px; border-radius:12px; border:0; background:#1f6feb; color:#fff; font-size:16px; }
    #log { position:fixed; left:8px; top:8px; color:#0f0; font:12px ui-monospace, SFMono-Regular, Menlo, Consolas, monospace; z-index:20; white-space:pre-wrap; max-width:90vw; }
    .wm { position:fixed; right:8px; bottom:8px; color:#fff9; font:12px ui-monospace, SFMono-Regular, Menlo, Consolas, monospace; }
  </style>
</head>
<body>
  <div id="ui"><div><h2>Face Filter</h2><p>Tap Start and allow the camera (front).</p><button id="start">Start</button></div></div>
  <div id="log"></div>
  <div class="wm">AR_EXAMPLE</div>
  <div id="stage">
    <video id="video" playsinline muted></video>
    <canvas id="canvas"></canvas>
  </div>

<script type="module">
import {FaceLandmarker, FilesetResolver} from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3";

const logEl = document.getElementById('log');
const log = (s)=> logEl.textContent = String(s ?? '');

const video = document.getElementById('video');
const canvas = document.getElementById('canvas');
const ctx = canvas.getContext('2d');
const stage = document.getElementById('stage');

function fit(){ canvas.width = stage.clientWidth; canvas.height = stage.clientHeight; }
window.addEventListener('resize', fit);

function rrRect(x,y,w,h,r){
  r = Math.min(r, w/2, h/2);
  ctx.beginPath();
  ctx.moveTo(x+r,y);
  ctx.arcTo(x+w,y,x+w,y+h,r);
  ctx.arcTo(x+w,y+h,x,y+h,r);
  ctx.arcTo(x,y+h,x,y,r);
  ctx.arcTo(x,y,x+w,y,r);
  ctx.closePath();
  ctx.fill();
}

function drawGlasses(landmarks) {
  const W = canvas.width, H = canvas.height;
  const toXY = p => ({ x: (1 - p.x) * W, y: p.y * H });

  const L = toXY(landmarks[263]), LI = toXY(landmarks[362]);
  const R = toXY(landmarks[33]),  RI = toXY(landmarks[133]);
  const leftC = { x:(L.x+LI.x)/2, y:(L.y+LI.y)/2 };
  const rightC= { x:(R.x+RI.x)/2, y:(R.y+RI.y)/2 };
  const dx = leftC.x - rightC.x, dy = leftC.y - rightC.y;
  const angle = Math.atan2(dy, dx);
  const eyeDist = Math.hypot(dx, dy);
  const lensW = eyeDist * 0.55, lensH = eyeDist * 0.32, bridgeW = eyeDist * 0.18;
  const rim = Math.max(2, Math.round(lensH * 0.12));
  const mid = { x:(leftC.x+rightC.x)/2, y:(leftC.y+rightC.y)/2 };
  const halfGap = bridgeW/2, lensGap = halfGap + lensW/2;

  ctx.save();
  ctx.translate(mid.x, mid.y);
  ctx.rotate(angle);

  ctx.fillStyle = 'rgba(15,15,15,0.94)';
  const rr = lensH*0.25;
  rrRect(-lensGap - lensW/2, -lensH/2, lensW, lensH, rr);
  rrRect(lensGap - lensW/2, -lensH/2, lensW, lensH, rr);

  ctx.fillStyle='rgba(20,20,20,0.95)';
  rrRect(-halfGap, -lensH*0.18, bridgeW, lensH*0.36, lensH*0.18);

  ctx.strokeStyle='rgba(10,10,10,0.9)';
  ctx.lineWidth=Math.max(2, rim*0.6);
  const armLen=lensW*0.9;
  ctx.beginPath();
  ctx.moveTo(-lensGap - lensW/2, -lensH*0.15); ctx.lineTo(-lensGap - lensW/2 - armLen, -lensH*0.15);
  ctx.moveTo(lensGap + lensW/2, -lensH*0.15);  ctx.lineTo(lensGap + lensW/2 + armLen, -lensH*0.15);
  ctx.stroke();

  ctx.restore();
}

async function start() {
  try {
    document.getElementById('ui').classList.add('hide');
    fit();
    if (!navigator.mediaDevices?.getUserMedia) {
      alert('getUserMedia not available (use HTTPS and a modern browser).');
      return;
    }
    const stream = await navigator.mediaDevices.getUserMedia({
      video: { facingMode: 'user', width:{ideal:1280}, height:{ideal:720} }, audio:false
    });
    video.srcObject = stream;
    await video.play();
    log('camera on');

    const fileset = await FilesetResolver.forVisionTasks(
      "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
    );

    const faceLandmarker = await FaceLandmarker.createFromOptions(fileset, {
      baseOptions: {
        modelAssetPath: "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm/face_landmarker.task"
      },
      runningMode: "VIDEO",
      numFaces: 1,
      outputFaceBlendshapes: false,
      outputFacialTransformationMatrixes: false
    });

    function render() {
      const startTs = Date.now();
      const result = faceLandmarker.detectForVideo(video, startTs);
      ctx.clearRect(0,0,canvas.width,canvas.height);
      if (result?.faceLandmarks && result.faceLandmarks.length > 0) {
        drawGlasses(result.faceLandmarks[0]);
        log('');
      } else {
        log('no face');
      }
      requestAnimationFrame(render);
    }
    requestAnimationFrame(render);
  } catch (e) {
    log('error: ' + e.message);
    alert('Camera failed: ' + e.message);
  }
}

document.getElementById('start').addEventListener('click', start);
</script>
</body>
</html>
